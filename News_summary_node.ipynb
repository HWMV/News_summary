{"cells":[{"cell_type":"markdown","metadata":{"id":"bRpfARC0hGRu"},"source":["# News_summary Project\n","# Architecture : LSTM, Attention\n","\n","### 온라인 6기 코어 최현우"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4801,"status":"ok","timestamp":1700016382866,"user":{"displayName":"mak huh","userId":"01097780720364492215"},"user_tz":-540},"id":"Ib1rb37ahWCS","outputId":"cc2e79d8-87ff-47fd-972c-92616e92315f"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"name":"stdout","output_type":"stream","text":["2.14.0\n"]}],"source":["import nltk\n","nltk.download('stopwords')\n","\n","import numpy as np\n","import pandas as pd\n","import os\n","import re\n","import matplotlib.pyplot as plt\n","from nltk.corpus import stopwords\n","from bs4 import BeautifulSoup\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import urllib.request\n","import warnings\n","warnings.filterwarnings(action='ignore')\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30228,"status":"ok","timestamp":1700016413085,"user":{"displayName":"mak huh","userId":"01097780720364492215"},"user_tz":-540},"id":"l3xpuoYyhV_c","outputId":"cb8a3201-9835-4a73-d9a2-50033b0ade43"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Google drive Mount\n","import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5432,"status":"ok","timestamp":1700016418515,"user":{"displayName":"mak huh","userId":"01097780720364492215"},"user_tz":-540},"id":"gKE0u_PjhWFK"},"outputs":[],"source":["# data load\n","data_path = '/content/drive/My Drive/News_summary/Reviews.csv'\n","data = pd.read_csv(data_path)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1700016418515,"user":{"displayName":"mak huh","userId":"01097780720364492215"},"user_tz":-540},"id":"f9cuXbMKhWIA","outputId":"02606234-34ba-4b48-ae07-2eb8d56fb89b"},"outputs":[{"name":"stdout","output_type":"stream","text":["전체 샘플 수:  568454\n"]}],"source":["# data samples Check\n","print('전체 샘플 수: ', (len(data)))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":678},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1700016418515,"user":{"displayName":"mak huh","userId":"01097780720364492215"},"user_tz":-540},"id":"C6EJlw2xhWKm","outputId":"7c81c837-218d-4424-a289-1deb780ca875"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-00fdb560-039e-4646-9632-133e4c6b5689\" class=\"colab-df-container\"\u003e\n","    \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eId\u003c/th\u003e\n","      \u003cth\u003eProductId\u003c/th\u003e\n","      \u003cth\u003eUserId\u003c/th\u003e\n","      \u003cth\u003eProfileName\u003c/th\u003e\n","      \u003cth\u003eHelpfulnessNumerator\u003c/th\u003e\n","      \u003cth\u003eHelpfulnessDenominator\u003c/th\u003e\n","      \u003cth\u003eScore\u003c/th\u003e\n","      \u003cth\u003eTime\u003c/th\u003e\n","      \u003cth\u003eSummary\u003c/th\u003e\n","      \u003cth\u003eText\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eB001E4KFG0\u003c/td\u003e\n","      \u003ctd\u003eA3SGXH7AUHU8GW\u003c/td\u003e\n","      \u003ctd\u003edelmartian\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e5\u003c/td\u003e\n","      \u003ctd\u003e1303862400\u003c/td\u003e\n","      \u003ctd\u003eGood Quality Dog Food\u003c/td\u003e\n","      \u003ctd\u003eI have bought several of the Vitality canned d...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003eB00813GRG4\u003c/td\u003e\n","      \u003ctd\u003eA1D87F6ZCVE5NK\u003c/td\u003e\n","      \u003ctd\u003edll pa\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e1346976000\u003c/td\u003e\n","      \u003ctd\u003eNot as Advertised\u003c/td\u003e\n","      \u003ctd\u003eProduct arrived labeled as Jumbo Salted Peanut...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003eB000LQOCH0\u003c/td\u003e\n","      \u003ctd\u003eABXLMWJIXXAIN\u003c/td\u003e\n","      \u003ctd\u003eNatalia Corres \"Natalia Corres\"\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e4\u003c/td\u003e\n","      \u003ctd\u003e1219017600\u003c/td\u003e\n","      \u003ctd\u003e\"Delight\" says it all\u003c/td\u003e\n","      \u003ctd\u003eThis is a confection that has been around a fe...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e4\u003c/td\u003e\n","      \u003ctd\u003eB000UA0QIQ\u003c/td\u003e\n","      \u003ctd\u003eA395BORC6FGVXV\u003c/td\u003e\n","      \u003ctd\u003eKarl\u003c/td\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e1307923200\u003c/td\u003e\n","      \u003ctd\u003eCough Medicine\u003c/td\u003e\n","      \u003ctd\u003eIf you are looking for the secret ingredient i...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e5\u003c/td\u003e\n","      \u003ctd\u003eB006K2ZZ7K\u003c/td\u003e\n","      \u003ctd\u003eA1UQRSCLF8GW1T\u003c/td\u003e\n","      \u003ctd\u003eMichael D. Bigham \"M. Wassir\"\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e5\u003c/td\u003e\n","      \u003ctd\u003e1350777600\u003c/td\u003e\n","      \u003ctd\u003eGreat taffy\u003c/td\u003e\n","      \u003ctd\u003eGreat taffy at a great price.  There was a wid...\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","    \u003cdiv class=\"colab-df-buttons\"\u003e\n","\n","  \u003cdiv class=\"colab-df-container\"\u003e\n","    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00fdb560-039e-4646-9632-133e4c6b5689')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n","    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","\n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","    \u003cscript\u003e\n","      const buttonEl =\n","        document.querySelector('#df-00fdb560-039e-4646-9632-133e4c6b5689 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-00fdb560-039e-4646-9632-133e4c6b5689');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","\n","\u003cdiv id=\"df-f4f5f41b-9ee3-4faf-a29f-cdd02b346302\"\u003e\n","  \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-f4f5f41b-9ee3-4faf-a29f-cdd02b346302')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\"\u003e\n","\n","\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\"\u003e\n","    \u003cg\u003e\n","        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n","    \u003c/g\u003e\n","\u003c/svg\u003e\n","  \u003c/button\u003e\n","\n","\u003cstyle\u003e\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","\u003c/style\u003e\n","\n","  \u003cscript\u003e\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() =\u003e {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-f4f5f41b-9ee3-4faf-a29f-cdd02b346302 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  \u003c/script\u003e\n","\u003c/div\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n"],"text/plain":["   Id   ProductId          UserId                      ProfileName  \\\n","0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n","1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n","2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n","3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n","4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n","\n","   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n","0                     1                       1      5  1303862400   \n","1                     0                       0      1  1346976000   \n","2                     1                       1      4  1219017600   \n","3                     3                       3      2  1307923200   \n","4                     0                       0      5  1350777600   \n","\n","                 Summary                                               Text  \n","0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n","1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n","2  \"Delight\" says it all  This is a confection that has been around a fe...  \n","3         Cough Medicine  If you are looking for the secret ingredient i...  \n","4            Great taffy  Great taffy at a great price.  There was a wid...  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# data top5 Check\n","data.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":763},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1700016418515,"user":{"displayName":"mak huh","userId":"01097780720364492215"},"user_tz":-540},"id":"2Uy8S63VhWSq","outputId":"db158476-bbdf-4088-d424-d3b913c8807e"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-e134c51a-c6fe-47f0-b720-28af3c450bf9\" class=\"colab-df-container\"\u003e\n","    \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eText\u003c/th\u003e\n","      \u003cth\u003eSummary\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e551829\u003c/th\u003e\n","      \u003ctd\u003eSo enjoyable in the middle of the afternoon......\u003c/td\u003e\n","      \u003ctd\u003eHOT OR COLD\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e519020\u003c/th\u003e\n","      \u003ctd\u003egreat candy, loved them in the late 70's and M...\u003c/td\u003e\n","      \u003ctd\u003ehard candy\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e405534\u003c/th\u003e\n","      \u003ctd\u003eLove this product, it is rich and creamy and t...\u003c/td\u003e\n","      \u003ctd\u003elittlelulu\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e133022\u003c/th\u003e\n","      \u003ctd\u003eThe title of this review says it all. There is...\u003c/td\u003e\n","      \u003ctd\u003eGreat coffee underneath vanilla alcohol aftert...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e315883\u003c/th\u003e\n","      \u003ctd\u003eI don't care if the flavor is fake. I love it ...\u003c/td\u003e\n","      \u003ctd\u003eLove it!\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e332849\u003c/th\u003e\n","      \u003ctd\u003eI appreciate the nutritional value of this pro...\u003c/td\u003e\n","      \u003ctd\u003eVery different taste and texture\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e89041\u003c/th\u003e\n","      \u003ctd\u003eI just ordered the 4-pack of this cereal and o...\u003c/td\u003e\n","      \u003ctd\u003e1 gram of sugar\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e388203\u003c/th\u003e\n","      \u003ctd\u003eThe Clear Hair shampoo had a nice clean scent....\u003c/td\u003e\n","      \u003ctd\u003eShampoo for normal to dry hair\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e322841\u003c/th\u003e\n","      \u003ctd\u003ePronounced olive flavor with lovely herbaceous...\u003c/td\u003e\n","      \u003ctd\u003eSmooth and silky\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e79284\u003c/th\u003e\n","      \u003ctd\u003eThis is the KIND bar that made me look into al...\u003c/td\u003e\n","      \u003ctd\u003eKIND chocolate, cherry, cashew\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e125310\u003c/th\u003e\n","      \u003ctd\u003eI couldn't get over the initial thought that t...\u003c/td\u003e\n","      \u003ctd\u003eGood once you get used to it...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e358714\u003c/th\u003e\n","      \u003ctd\u003eWas getting VitaCoco, decided to try this bran...\u003c/td\u003e\n","      \u003ctd\u003eWeird taste, maybe spoiled?\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e288248\u003c/th\u003e\n","      \u003ctd\u003eI tried these on a recommendation from the aut...\u003c/td\u003e\n","      \u003ctd\u003eSatisfies sweet tooth without sugar!\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e362868\u003c/th\u003e\n","      \u003ctd\u003eFresh popcorn seed, sea salt and no packaging ...\u003c/td\u003e\n","      \u003ctd\u003eFunny name great product\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e85701\u003c/th\u003e\n","      \u003ctd\u003eItems were just as described. The dogs went ab...\u003c/td\u003e\n","      \u003ctd\u003eGreat, but I'd hoped they'd last longer\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","    \u003cdiv class=\"colab-df-buttons\"\u003e\n","\n","  \u003cdiv class=\"colab-df-container\"\u003e\n","    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e134c51a-c6fe-47f0-b720-28af3c450bf9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n","    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","\n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","    \u003cscript\u003e\n","      const buttonEl =\n","        document.querySelector('#df-e134c51a-c6fe-47f0-b720-28af3c450bf9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e134c51a-c6fe-47f0-b720-28af3c450bf9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","\n","\u003cdiv id=\"df-1ef11170-6f4f-42b7-9fe4-2b6c9532c4b3\"\u003e\n","  \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-1ef11170-6f4f-42b7-9fe4-2b6c9532c4b3')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\"\u003e\n","\n","\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\"\u003e\n","    \u003cg\u003e\n","        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n","    \u003c/g\u003e\n","\u003c/svg\u003e\n","  \u003c/button\u003e\n","\n","\u003cstyle\u003e\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","\u003c/style\u003e\n","\n","  \u003cscript\u003e\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() =\u003e {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-1ef11170-6f4f-42b7-9fe4-2b6c9532c4b3 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  \u003c/script\u003e\n","\u003c/div\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n"],"text/plain":["                                                     Text  \\\n","551829  So enjoyable in the middle of the afternoon......   \n","519020  great candy, loved them in the late 70's and M...   \n","405534  Love this product, it is rich and creamy and t...   \n","133022  The title of this review says it all. There is...   \n","315883  I don't care if the flavor is fake. I love it ...   \n","332849  I appreciate the nutritional value of this pro...   \n","89041   I just ordered the 4-pack of this cereal and o...   \n","388203  The Clear Hair shampoo had a nice clean scent....   \n","322841  Pronounced olive flavor with lovely herbaceous...   \n","79284   This is the KIND bar that made me look into al...   \n","125310  I couldn't get over the initial thought that t...   \n","358714  Was getting VitaCoco, decided to try this bran...   \n","288248  I tried these on a recommendation from the aut...   \n","362868  Fresh popcorn seed, sea salt and no packaging ...   \n","85701   Items were just as described. The dogs went ab...   \n","\n","                                                  Summary  \n","551829                                        HOT OR COLD  \n","519020                                         hard candy  \n","405534                                         littlelulu  \n","133022  Great coffee underneath vanilla alcohol aftert...  \n","315883                                           Love it!  \n","332849                   Very different taste and texture  \n","89041                                     1 gram of sugar  \n","388203                     Shampoo for normal to dry hair  \n","322841                                   Smooth and silky  \n","79284                      KIND chocolate, cherry, cashew  \n","125310                    Good once you get used to it...  \n","358714                        Weird taste, maybe spoiled?  \n","288248               Satisfies sweet tooth without sugar!  \n","362868                           Funny name great product  \n","85701             Great, but I'd hoped they'd last longer  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# df Text \u0026 Summary column\n","# data 변수에 2개의 컬럼만 선택\n","data = data[['Text','Summary']]\n","data.head()\n","\n","data.sample(15)"]},{"cell_type":"markdown","metadata":{"id":"4KNdS63Blryp"},"source":["# 1. Data preprocessing"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1224,"status":"ok","timestamp":1700016419734,"user":{"displayName":"mak huh","userId":"01097780720364492215"},"user_tz":-540},"id":"KFFS2htAkBdK","outputId":"a06fa47d-3748-485f-e2b6-2bac286576d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Text 열에서 중복을 배제한 유일한 샘플의 수 : 393579\n","Summary 열에서 중복을 배제한 유일한 샘플의 수 : 295742\n"]}],"source":["# 데이터 중복 샘플 유무 확인 (nuique() 활용)\n","\n","print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n","print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1123,"status":"ok","timestamp":1700016420856,"user":{"displayName":"mak huh","userId":"01097780720364492215"},"user_tz":-540},"id":"Vfam92HtkBfz","outputId":"8e9b5361-55dd-4ce2-987c-1c6915509b6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["전체 샘플수 : 393579\n"]}],"source":["# Text 자체가 중복인 샘플 제거 (drop_duplicates() 활용)- 기억! 처음 봄\n","#\n","# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다\n","data.drop_duplicates(subset = ['Text'], inplace=True)\n","print('전체 샘플수 :', (len(data)))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1700016420857,"user":{"displayName":"mak huh","userId":"01097780720364492215"},"user_tz":-540},"id":"fioSX23-kBiG","outputId":"bfee7978-de22-45ff-8b97-407c3b5c0d4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Text       0\n","Summary    3\n","dtype: int64\n"]}],"source":["# data에 Null 값 잔여 여부 확인\n","print(data.isnull().sum())"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1700016420857,"user":{"displayName":"mak huh","userId":"01097780720364492215"},"user_tz":-540},"id":"23_Xqv3UhWVy","outputId":"f3e55d70-eeac-45a3-88cc-a89b418e8a1b"},"outputs":[{"name":"stdout","output_type":"stream","text":["전체 샘플수 : 393576\n"]}],"source":["# Summary 3개 Null delete (dropna())\n","data.dropna(axis=0, inplace=True)\n","print('전체 샘플수 :', (len(data)))"]},{"cell_type":"markdown","metadata":{"id":"DYa3nbbLm8vm"},"source":["# 2. 텍스트 정규화 \u0026 불용어 제거\n","같은 의미이나 형태가 다른 것을 러닝 전에 통일 시켜주는 작업을 '정규화' 라고 한다\n","\n","'불용어'는 NLP에서 별 도움이 되지 않는 단어 입니다"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1700016420857,"user":{"displayName":"mak huh","userId":"01097780720364492215"},"user_tz":-540},"id":"g6KjMz_1nAJE","outputId":"ee4ff688-b913-47d0-8908-752b85b471c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["정규화 사전의 수:  120\n"]}],"source":["# Text normalization\n","# 사람이 직접 자연어 처리를 위해 만든 사전..\n","contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n","                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n","                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n","                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n","                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n","                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n","                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n","                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n","                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n","                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n","                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n","                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n","                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n","                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n","                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n","                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n","                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n","                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n","                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n","                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n","                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n","                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n","                           \"you're\": \"you are\", \"you've\": \"you have\"}\n","\n","print(\"정규화 사전의 수: \", len(contractions))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1700016420857,"user":{"displayName":"mak huh","userId":"01097780720364492215"},"user_tz":-540},"id":"cJFcdf8WnALq","outputId":"8da479ca-c72d-4709-822e-d2964fcf4b49"},"outputs":[{"name":"stdout","output_type":"stream","text":["불용어 개수 : 179\n","['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}],"source":["# 불용어 제거, NLTK에서 제공하는 불용어 갯수 (stopwords 기억! 처음 봄, 영어로 설정?)\n","print('불용어 개수 :', len(stopwords.words('english') ))\n","print(stopwords.words('english'))"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1700016420857,"user":{"displayName":"mak huh","userId":"01097780720364492215"},"user_tz":-540},"id":"TRzz9158nAOg","outputId":"76327e2f-c211-4a60-c006-23988472b467"},"outputs":[{"name":"stdout","output_type":"stream","text":["=3\n"]}],"source":["# 여러가지 자연어 전처리 함수 기억!-외우기\n","# 데이터 전처리 함수\n","def preprocess_sentence(sentence, remove_stopwords=True):\n","    sentence = sentence.lower() # 텍스트 소문자화\n","    sentence = BeautifulSoup(sentence, \"lxml\").text # \u003cbr /\u003e, \u003ca href = ...\u003e 등의 html 태그 제거\n","    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for =\u003e my husband for\n","    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n","    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n","    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -\u003e roland\n","    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n","    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -\u003e umm yeah\n","\n","    # 불용어 제거 (Text)\n","    if remove_stopwords:\n","        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) \u003e 1)\n","    # 불용어 미제거 (Summary)\n","    else:\n","        tokens = ' '.join(word for word in sentence.split() if len(word) \u003e 1)\n","    return tokens\n","print('=3')"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1700016420857,"user":{"displayName":"mak huh","userId":"01097780720364492215"},"user_tz":-540},"id":"KAxeCzutnAQ4","outputId":"ff3bc724-e24d-4ef4-9d42-11b2aa520d0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["text:  everything bought great infact ordered twice third ordered wasfor mother father\n","summary: great way to start the day\n"]}],"source":["# 전처리 전,후의 결과 체크, 임의의 text, summary 호출 함수\n","temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was\u003cbr /\u003efor my mother and father.'\n","temp_summary = 'Great way to start (or finish) the day!!!'\n","\n","print(\"text: \", preprocess_sentence(temp_text))\n","print(\"summary:\", preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다.\n","\n","# 요약이 제대로 된거 맞나요???"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tM0cu0nSnATv"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-15-00d0707e5261\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 6\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 위 함수로 불용어 제거\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 7\u001b[0;31m   \u001b[0mclean_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 전처리 후 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-13-ce0135db39ba\u003e\u001b[0m in \u001b[0;36mpreprocess_sentence\u001b[0;34m(sentence, remove_stopwords)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 불용어 제거 (Text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 15\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# 불용어 미제거 (Summary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-13-ce0135db39ba\u003e\u001b[0m in \u001b[0;36m\u003cgenexpr\u003e\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 불용어 제거 (Text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 15\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# 불용어 미제거 (Summary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 19\u001b[0;31m         return [\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m\u003clistcomp\u003e\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 19\u001b[0;31m         return [\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Train data 전처리\n","# Text만 불용어를 제거할 예정\n","clean_text = []\n","\n","# 위 함수로 불용어 제거\n","for x in data['Text']:\n","  clean_text.append(preprocess_sentence(x))\n","\n","# 전처리 후 출력\n","print(\"Text 전처리 후 결과: \", clean_text[:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z9alrt-mnAVQ"},"outputs":[],"source":["# 전체 Summary 데이터에 대한 전처리\n","clean_summary = []\n","\n","# 전체 Summary 데이터에 대한 전처리\n","for x in data['Summary']:\n","    # preprocess_sentence 함수를 호출하되, remove_stopwords 인자를 False로 설정\n","    clean_summary.append(preprocess_sentence(x, remove_stopwords=False))\n","\n","print(\"Summary 전처리 후 결과: \", clean_summary[:5])"]},{"cell_type":"markdown","metadata":{"id":"x1PuYPbx9i3U"},"source":["****\n","\u0026\u0026\u0026runtime : 모델 runtime 다시할 때 경계선 (전처리 너무 오래걸림..)\n","****"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDhxc0gDnAdF"},"outputs":[],"source":["data['Text'] = clean_text\n","data['Summary'] = clean_summary\n","\n","# 빈 값을 Null 값으로 변환\n","data.replace('', np.nan, inplace=True)\n","print('=3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cVehgKkfnAfr"},"outputs":[],"source":["# 전처리 하면서 생긴 Null 값 확인\n","data.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rUIeN6y5nAhR"},"outputs":[],"source":["# 새로 생긴 Null value 제거\n","data.dropna(axis=0, inplace=True)\n","print('전체 샘플수 :', (len(data)))"]},{"cell_type":"markdown","metadata":{"id":"98rcf0Jx5nym"},"source":["# 3. Data preprocessing 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VkLlEK2U5rg6"},"outputs":[],"source":["# train data/test data Separation\n","# Sample max len (훈련에 사용할 샘플의 최대 길이 정하기)\n","# 길이 분포 출력\n","import matplotlib.pyplot as plt\n","\n","text_len = [len(s.split()) for s in data['Text']]\n","summary_len = [len(s.split()) for s in data['Summary']]\n","\n","print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n","print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n","print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n","print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n","print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n","print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n","\n","plt.subplot(1,2,1)\n","plt.boxplot(text_len)\n","plt.title('Text')\n","plt.subplot(1,2,2)\n","plt.boxplot(summary_len)\n","plt.title('Summary')\n","plt.tight_layout()\n","plt.show()\n","\n","plt.title('Text')\n","plt.hist(text_len, bins = 40)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()\n","\n","plt.title('Summary')\n","plt.hist(summary_len, bins = 40)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzHUDTYO5rje"},"outputs":[],"source":["# Text의 최대 길이와 Summary의 적절한 최대 길이 임의 설정\n","text_max_len = 125\n","summary_max_len = 13\n","print('=3')"]},{"cell_type":"markdown","metadata":{"id":"cbnCOmeZ6YFQ"},"source":["****\n","### * 각각 50과 8로 정했는데 이 길이를 선택했을 때, 얼마나 많은 샘플들을 자르지 않고 포함할 수 있는지 통계로 확인하는 편이 객관적으로 길이를 결정하는 데 도움이 될거예요. 훈련 데이터와 샘플의 길이를 입력하면, 데이터의 몇 %가 해당하는지 계산하는 함수를 만들어서 좀 더 정확하게 판단해보자\n","****"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzpm2x1t5rmH"},"outputs":[],"source":["# 길이에 따른 데이터가 샘플의 몇 % 인지 확인하는 함수\n","def below_threshold_len(max_len, nested_list):\n","  cnt = 0\n","  for s in nested_list:\n","    if(len(s.split()) \u003c= max_len):\n","        cnt = cnt + 1\n","  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n","print('=3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MndBg0Tm5ro_"},"outputs":[],"source":["# Text \u0026 Summary에 위 함수 적용하기\n","below_threshold_len(text_max_len, data['Text'])\n","below_threshold_len(summary_max_len,  data['Summary'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DETXAK2F5rrU"},"outputs":[],"source":["# text_max_len \u0026 summary_max_len의 길이보다 큰 샘플을 제외\n","# 'Text' 열의 각 샘플의 길이가 text_max_len 이하인지 확인\n","data = data[data['Text'].apply(lambda x: len(x.split()) \u003c= text_max_len)]\n","\n","# 'Summary' 열의 각 샘플의 길이가 summary_max_len 이하인지 확인\n","data = data[data['Summary'].apply(lambda x: len(x.split()) \u003c= summary_max_len)]\n","\n","print('전체 샘플수 :', (len(data)))"]},{"cell_type":"markdown","metadata":{"id":"kWNSnGNY82P3"},"source":["# 시작 토큰(SOS), 종료 토큰(EOS) 추가\n","디코더 부분에서의 개념\n","* 디코더는 시작 토큰을 입력받아 문장을 생성하기 시작하고, 종료 토큰을 예측한 순간에 문장 생성을 멈추는 것"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6h02kjXi5rtp"},"outputs":[],"source":["# SOS, EOS 추가\n","# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n","data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n","data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gE2dLSGV-SVG"},"outputs":[],"source":["# 디코더의 SOS, EOS 를 각각 np Type save\n","encoder_input = np.array(data['Text']) # 인코더의 입력\n","decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n","decoder_target = np.array(data['decoder_target']) # 디코더의 레이블\n","print('=3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhcjJnX6-doD"},"outputs":[],"source":["# train/test data separation\n","# shuffle (샘플 섞기)\n","indices = np.arange(encoder_input.shape[0])\n","np.random.shuffle(indices)\n","print(indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ktcfdck-8My"},"outputs":[],"source":["# shuffle data 대입\n","encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_target = decoder_target[indices]\n","print('=3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OxrGlf6g_JiU"},"outputs":[],"source":["# encorder에 들어갈 test data separation(20%)\n","\n","n_of_val = int(len(encoder_input)*0.2)\n","print('테스트 데이터의 수 :', n_of_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWnFyK6D_Ziv"},"outputs":[],"source":["# train, test data separation Check\n","encoder_input_train = encoder_input[:-n_of_val]\n","decoder_input_train = decoder_input[:-n_of_val]\n","decoder_target_train = decoder_target[:-n_of_val]\n","\n","encoder_input_test = encoder_input[-n_of_val:]\n","decoder_input_test = decoder_input[-n_of_val:]\n","decoder_target_test = decoder_target[-n_of_val:]\n","\n","print('훈련 데이터의 개수 :', len(encoder_input_train))\n","print('훈련 레이블의 개수 :', len(decoder_input_train))\n","print('테스트 데이터의 개수 :', len(encoder_input_test))\n","print('테스트 레이블의 개수 :', len(decoder_input_test))"]},{"cell_type":"markdown","metadata":{"id":"qcQaIdUVO4BW"},"source":["# 4-1. 데이터 전처리\n","단어 집합(Vocabulary) 만들기 \u0026 정수 인코딩\n","중요한 부분이라 챕터 나눔\n","\n","* Vocalbulary를 만든다는 것은\n","단어 : 빈도 수 쌍으로 이루어진 딕셔너리 생성\n","\n","* 빈도 수를 가지고 특정 빈도의 희귀 단어를 제외하는 전처리 수행(오버피팅 방지)"]},{"cell_type":"markdown","metadata":{"id":"Ict8-AvGWDFh"},"source":["* Encoder input data 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JYiCcA2TO2L8"},"outputs":[],"source":["# encoder_input_train vocabulary\n","src_tokenizer = Tokenizer() # 토크나이저 정의\n","src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성\n","print('=3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yk9Wc7cZPCvG"},"outputs":[],"source":["# src_tokenizer.word_counts.items()에는 단어와 각 단어의 등장 빈도수가 저장\n","# 위를 통해서 통계적인 정보를 얻기\n","# 빈도수 N회 미만인 단어의 비중 확인\n","threshold = 7\n","total_cnt = len(src_tokenizer.word_index) # 단어의 수\n","rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n","total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n","rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n","\n","# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n","for key, value in src_tokenizer.word_counts.items():\n","    total_freq = total_freq + value\n","\n","    # 단어의 등장 빈도수가 threshold보다 작으면\n","    if(value \u003c threshold):\n","        rare_cnt = rare_cnt + 1\n","        rare_freq = rare_freq + value\n","\n","print('단어 집합(vocabulary)의 크기 :', total_cnt)\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print('단어 집합에서 희귀 단어를 제외 시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n","print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_8ltPvsPCxm"},"outputs":[],"source":["# 희귀 단어 등장 빈도가 상당히 적기에 제외 하고 훈련\n","src_vocab = 17000\n","src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 17,000으로 제한\n","src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성\n","print('=3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xh0K8eYgPCz8"},"outputs":[],"source":["# vocabulary 정수 인코딩 수행. voca 크기가 17000 초과는 인코딩 후\n","# 데이터에 존재하지 않음\n","# 텍스트 시퀀스를 정수 시퀀스로 변환\n","encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train)\n","encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n","\n","# 잘 진행되었는지 샘플 출력\n","print(encoder_input_train[:3])"]},{"cell_type":"markdown","metadata":{"id":"u2XIm2UUV_5Z"},"source":["* Decoder input data preprocessing\n","\n","Column 2개 생성 : Summary에 각각 SOS, EOS 추가\n","\n","? train, test가 각 컬럴별로 데이터를 분리해 놓은 상태가 맞는건가?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LL5V7lfmPC3B"},"outputs":[],"source":["# Summary data 위와 동일한 작업 수행\n","tar_tokenizer = Tokenizer()\n","tar_tokenizer.fit_on_texts(decoder_input_train)\n","print('=3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yr1ntN8iPC5I"},"outputs":[],"source":["# 위 과정 반복 Summary\n","threshold = 6\n","total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n","rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n","total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n","rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n","\n","# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n","for key, value in tar_tokenizer.word_counts.items():\n","    total_freq = total_freq + value\n","\n","    # 단어의 등장 빈도수가 threshold보다 작으면\n","    if(value \u003c threshold):\n","        rare_cnt = rare_cnt + 1\n","        rare_freq = rare_freq + value\n","\n","print('단어 집합(vocabulary)의 크기 :', total_cnt)\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n","print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0N2VlrphaRF4"},"outputs":[],"source":["# 동일하게 집합의 크기 제한 (희귀 단어를 제외해야 오버피팅 방지)\n","tar_vocab = 5700\n","tar_tokenizer = Tokenizer(num_words=tar_vocab)\n","tar_tokenizer.fit_on_texts(decoder_input_train)\n","tar_tokenizer.fit_on_texts(decoder_target_train)\n","\n","# 텍스트 시퀀스를 정수 시퀀스로 변환\n","decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train)\n","decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n","decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n","decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n","\n","# 잘 변환되었는지 확인\n","print('input')\n","print('input ',decoder_input_train[:5])\n","print('target')\n","print('decoder ',decoder_target_train[:5])"]},{"cell_type":"markdown","metadata":{"id":"xJ-mOHWhaki6"},"source":["**********\n","# 정수 인코딩 작업 완료\n","\n","단어를 제한한다 : 빈도수가 가장 높은 N개의 단어까지만 사용하겠다 라는 의미, 자동으로 희귀 빈도 단어 수를 빼면 높은 빈도의 단어들만 사용한다는 의미"]},{"cell_type":"markdown","metadata":{"id":"sKcq7sX1d0wU"},"source":["### 1. Summary에서 길이가 0이 된 샘플들의 인덱스를 받기\n","### 2. 주의할 점은 요약문인 decoder_input에는 sostoken 또는 decoder_target에는 eostoken이 추가된 상태\n","### 3. 이 두 토큰은 모든 샘플에서 등장하므로 빈도수가 샘플 수와 동일하게 매우 높으므로 단어 집합 제한에도 삭제되지 않습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tICkYxoahcP"},"outputs":[],"source":["# 제한한 빈도수의 단어들은 Null 값으로 변형 되었을 수도 있다\n","# 그래서 확인 후 제거 (len=0 된 샘플 인덱스 호출)\n","# len = 1 기준은 SOS, EOS 단어 1개씩 추가 되었기 때문?\n","drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n","drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n","\n","print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n","print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n","\n","encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n","decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n","decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n","\n","encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n","decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n","decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n","\n","print('훈련 데이터의 개수 :', len(encoder_input_train))\n","print('훈련 레이블의 개수 :', len(decoder_input_train))\n","print('테스트 데이터의 개수 :', len(encoder_input_test))\n","print('테스트 레이블의 개수 :', len(decoder_input_test))"]},{"cell_type":"markdown","metadata":{"id":"zO3qH0UVejV1"},"source":["# 4-2. 데이터 전처리 (Padding)\n","\n","* 이제 서로 다른 길이의 샘플들을 병렬 처리하기 위해 같은 길이로 맞춰주는 패딩 작업\n","\n","* 이 전에 정의 해두었던 최대 길이로 패팅\n","(text_max_len, summary_max_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yr0ZHUpsahe0"},"outputs":[],"source":["# 정의 해둔 최대 길이로 Text, Summary Padding\n","encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n","encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n","decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n","decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n","decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n","decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')\n","print('=3')"]},{"cell_type":"markdown","metadata":{"id":"r75Ch2b9e9pZ"},"source":["******\n","# 전처리 모두 완료!\n","******"]},{"cell_type":"markdown","metadata":{"id":"Kb7tY_p1fOrd"},"source":["# 5. 모델 설계하기\n","* LSTM 구조"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SUc1QaoJahhI"},"outputs":[],"source":["from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","# LSTM input size 3차원 (data_size, time_step, feature)\n","# 인코더 설계 시작\n","# 벡터의 차원 128, hs 크기 256 (hs는 모델에서 얼만큼의 수용력에 대한 파라미터)\n","embedding_dim = 128\n","hidden_size = 256\n","\n","# 인코더\n","encoder_inputs = Input(shape=(text_max_len,))\n","\n","# 인코더의 임베딩 층\n","enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n","\n","# 인코더의 LSTM 1\n","# recurrent_dropout은 레이어가 아닌 스텝마다 적용 (오버피팅 방지) : Variational Dropout\n","# encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n","encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4)\n","encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n","\n","# 인코더의 LSTM 2\n","encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n","encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n","\n","# 인코더의 LSTM 3\n","encoder_lstm3 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n","encoder_outputs, state_h3, state_c3 = encoder_lstm3(encoder_output2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05ra-3CEahjv"},"outputs":[],"source":["# 디코더 설계\n","decoder_inputs = Input(shape=(None,))\n","\n","# 디코더의 임베딩 층\n","dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n","dec_emb = dec_emb_layer(decoder_inputs)\n","\n","# 디코더의 LSTM\n","# decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n","decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n","decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h3, state_c3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7sD81_z7niXu"},"outputs":[],"source":["# 디코더 출력 \u0026 모델 정의\n","# 디코더의 출력층\n","decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n","decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs)\n","\n","# 모델 정의\n","model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L6T8FZqcpMja"},"outputs":[],"source":["# Model Summary 시각화\n","from tensorflow.keras.utils import plot_model\n","\n","plot_model(model, show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","metadata":{"id":"5EuvuTjyol7P"},"source":["# 5-1. Attention Add\n","\n","Attention function + Decoder = Output\n","\n","여기서는 Bahdanau Style 활용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ukv6ROKjp7pb"},"outputs":[],"source":["from tensorflow.keras.layers import AdditiveAttention\n","\n","# 어텐션 층(어텐션 함수)\n","attn_layer = AdditiveAttention(name='attention_layer')\n","\n","# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n","attn_out = attn_layer([decoder_outputs, encoder_outputs])\n","\n","\n","# 어텐션의 결과와 디코더의 hidden state들을 연결\n","decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n","\n","# 디코더의 출력층\n","decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n","decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n","\n","# 모델 정의\n","model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"Upd4sI9stLwr"},"source":["# 6. Model Fit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uNtvp3T8p8So"},"outputs":[],"source":["# model compile\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n","history = model.fit(x=[encoder_input_train, decoder_input_train],\n","                    y=decoder_target_train,\n","                    validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n","                    batch_size=512, callbacks=[es], epochs=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vq8fJF-H14CA"},"outputs":[],"source":["# 시각화\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Test Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Loss')\n","plt.ylim([0.5,1.5])\n","plt.title('Training and Test Loss')\n","plt.xlabel('epoch')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"umnRKPR52pFh"},"source":["# 7. 인퍼런스 모델 구현\n","# 인퍼런스 모델 구현은 좀 더 심도 있게 학습해보자!\n","\n","* 테스트 단계에서는 정수 인덱스 행렬로 존재하던 텍스트 데이터를 실제 데이터로 복원해야 하므로, 필요한 3개의 사전을 아래와 같이 미리 준비"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-RktDvDp8bf"},"outputs":[],"source":["# 인코딩 해서 정수로 된 Voca에서 입력으로 쓸 원본 단어로 변경\n","\n","src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -\u003e 단어를 얻음\n","tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -\u003e 정수를 얻음\n","tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -\u003e 단어를 얻음\n","\n","print('=3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCHbBVESp8VP"},"outputs":[],"source":["# ! Seq2Seq는 훈련 시와 동작할 때(인퍼런스)의 방식이 다르므로 그에 맞게 모델 설계를 별개로 해야 합니다\n","# 인퍼런스 단계에서는 인코더/디코더 모델을 분리해서 설계 해야 합니다 (디코더가 입력만큼 반복해야 되기 때문에)\n","\n","# 인코더 설계\n","encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h3, state_c3])\n","\n","# 이전 시점의 상태들을 저장하는 텐서\n","decoder_state_input_h = Input(shape=(hidden_size,))\n","decoder_state_input_c = Input(shape=(hidden_size,))\n","\n","dec_emb2 = dec_emb_layer(decoder_inputs)\n","\n","# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n","# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n","decoder_outputs2, state_h3, state_c3 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n","\n","print('=3')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hfq5xDdjp8Xl"},"outputs":[],"source":["# 어텐션 함수\n","decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n","attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n","decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n","\n","# 디코더의 출력층\n","decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat)\n","\n","# 최종 디코더 모델\n","decoder_model = Model(\n","    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n","    [decoder_outputs2] + [state_h3, state_c3])\n","\n","print('=3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SB5B0WQa70tA"},"outputs":[],"source":["def decode_sequence(input_seq):\n","    # 입력으로부터 인코더의 상태를 얻음\n","    e_out, e_h, e_c = encoder_model.predict(input_seq)\n","\n","     # \u003cSOS\u003e에 해당하는 토큰 생성\n","    target_seq = np.zeros((1,1))\n","    target_seq[0, 0] = tar_word_to_index['sostoken']\n","\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n","\n","        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_token = tar_index_to_word[sampled_token_index]\n","\n","        if (sampled_token!='eostoken'):\n","            decoded_sentence += ' '+sampled_token\n","\n","        #  \u003ceos\u003e에 도달하거나 최대 길이를 넘으면 중단.\n","        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) \u003e= (summary_max_len-1)):\n","            stop_condition = True\n","\n","        # 길이가 1인 타겟 시퀀스를 업데이트\n","        target_seq = np.zeros((1,1))\n","        target_seq[0, 0] = sampled_token_index\n","\n","        # 상태를 업데이트 합니다.\n","        e_h, e_c = h, c\n","\n","    return decoded_sentence\n","print('=3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4WXi_PhM70vL"},"outputs":[],"source":["# seq2summary if 부분 다시 한번 공부\n","# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq2text(input_seq):\n","    temp=''\n","    for i in input_seq:\n","        if (i!=0):\n","            temp = temp + src_index_to_word[i]+' '\n","    return temp\n","\n","# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq2summary(input_seq):\n","    # [[YOUR CODE]]\n","    # 조건 숫자 0, SOS, EOS 제외\n","    temp=''\n","    for i in input_seq:\n","        # 위에서 lambda 식으로 decoder 각 컬럼에 SOS, EOS 추가 함\n","        if (i !=0 and i != tar_word_to_index['sostoken'] and i != tar_word_to_index['eostoken']):\n","            temp += tar_index_to_word[i] + ''\n","    return temp\n","\n","print('=3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkgrSsx5_CA-"},"outputs":[],"source":["# 샘플 10개만 실험\n","for i in range(50, 60):\n","    print(\"원문 :\", seq2text(encoder_input_test[i]))\n","    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n","    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n","    print(\"\\n\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOvG+52s8gO2kSxye5duqEW","name":"","provenance":[{"file_id":"1kPl9C0yntpsmc9F8U_kMYsafbwBEfHF2","timestamp":1699951797518}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}